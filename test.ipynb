{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Codes\n",
    "```\n",
    "set(['a', 'b']).issubset(['a', 'b', 'c'])\n",
    "df[df['class'].str.contains('PHL_', na=False)]\n",
    "df[df['class'].str.contains('PHL_|KD', regex=True, na=False)] ################# image\n",
    "df1[(~df1.m.isin(df2.m))&(~df1.y.isin(df2.y))] # &(~df1.col2.isin(common.col2))\n",
    "\n",
    "df1 = pd.DataFrame(data = {'A' : [1, 2, 3, 4, 5, 3], \n",
    "                           'B' : [10, 11, 12, 13, 14, 10]}) \n",
    "df2 = pd.DataFrame(data = {'A' : [1, 2, 3],\n",
    "                           'B' : [10, 11, 12]})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スライスレベル記載有り\n",
    "```\n",
    "if not re.search(\",|、\", slice_level):\n",
    "    ed_bool, edited_level = self.__slicelevel_calculator(slice_level)\n",
    "\n",
    "    if ed_bool and (not edited_level):\n",
    "        copy_labels_df = copy_labels_df.drop(i)\n",
    "        self.lvlarrng_log.warning(\"「%s」テストラベルのスライスレベルの間違い。\", test_label)\n",
    "        continue\n",
    "    else:\n",
    "        # スライスレベルが演算\n",
    "        copy_labels_df.loc[i, [\"スライスレベル\"]] = edited_level\n",
    "\n",
    "    copy_labels_df.loc[i, [\"temp_testlbl\"]] = test_label\n",
    "\n",
    "else:\n",
    "    # スライスレベルが複数\n",
    "    copy_labels_df.loc[i, [\"many\"]] = True\n",
    "    editedlbl_lst, whole_lbls, edited_df = self.__many_level_operator(copy_labels_df, test_label,\n",
    "                                                                    slice_level)\n",
    "\n",
    "    if not editedlbl_lst:\n",
    "        copy_labels_df = copy_labels_df.drop(i)\n",
    "        self.lvlarrng_log.warning(\"「%s」はテストラベルは紐づけ出来ません。\", test_label)\n",
    "        continue\n",
    "    else:\n",
    "        copy_labels_df.loc[i, [\"適用後\"]] = editedlbl_lst\n",
    "        copy_labels_df.loc[i, [\"temp_testlbl\"]] = whole_lbls\n",
    "        # copy_labels_df = pd.concat([copy_labels_df, edited_df], ignore_index=True)\n",
    "        copy_labels_df = pd.concat([copy_labels_df, edited_df])\n",
    "        self.lvlarrng_log.info(\"「%s」テストラベルの「%s」はテストラベルリストは紐づけ出来ます。\", test_label, editedlbl_lst)\n",
    "```\n",
    "\n",
    "### スライスレベル記載なし\n",
    "```\n",
    "if not re.search(\",|、\", new_slice_level):\n",
    "    ed_bool, edited_level = self.__slicelevel_calculator(new_slice_level)\n",
    "\n",
    "    if ed_bool and (not edited_level):\n",
    "        copy_labels_df = copy_labels_df.drop(i)\n",
    "        self.lvlarrng_log.warning(\"「%s」テストラベルのスライスレベルの間違い。\", test_label)\n",
    "        continue\n",
    "    else:\n",
    "        # スライスレベルが演算\n",
    "        copy_labels_df.loc[i, [\"スライスレベル\"]] = edited_level\n",
    "\n",
    "    copy_labels_df.loc[i, [\"temp_testlbl\"]] = test_label\n",
    "\n",
    "else:\n",
    "    copy_labels_df.loc[i, [\"many\", \"スライスレベル\"]] = [True, new_slice_level]\n",
    "\n",
    "    editedlbl_lst, whole_lbls, edited_df = self.__many_level_operator(copy_labels_df, test_label,\n",
    "                                                                        new_slice_level)\n",
    "\n",
    "    if not editedlbl_lst:\n",
    "        copy_labels_df = copy_labels_df.drop(i)\n",
    "        self.lvlarrng_log.warning(\"「%s」はテストラベルは紐づけ出来ません。\", test_label)\n",
    "        continue\n",
    "    else:\n",
    "        copy_labels_df.loc[i, [\"適用後\"]] = editedlbl_lst\n",
    "        copy_labels_df.loc[i, [\"temp_testlbl\"]] = whole_lbls\n",
    "        # copy_labels_df = pd.concat([copy_labels_df, edited_df], ignore_index=True)\n",
    "        copy_labels_df = pd.concat([copy_labels_df, edited_df])\n",
    "        self.lvlarrng_log.info(\"「%s」テストラベルの「%s」はテストラベルリストは紐づけ出来ます。\", test_label, editedlbl_lst)\n",
    "```\n",
    "\n",
    "### Defectラベル無し\n",
    "\n",
    "```\n",
    "if not re.search(\",|、\", temp_rw_level):\n",
    "    ed_bool, edited_level = self.__slicelevel_calculator(temp_rw_level)\n",
    "\n",
    "    if ed_bool and (not edited_level):\n",
    "        copy_labels_df = copy_labels_df.drop(i)\n",
    "        self.lvlarrng_log.warning(\"「%s」テストラベルのスライスレベルの間違い。\", test_label)\n",
    "        continue\n",
    "    else:\n",
    "        # スライスレベルが演算\n",
    "        copy_labels_df.loc[i, [\"スライスレベル\"]] = edited_level\n",
    "\n",
    "    copy_labels_df.loc[i, [\"temp_testlbl\"]] = test_label\n",
    "\n",
    "else:\n",
    "    copy_labels_df.loc[i, [\"many\", \"スライスレベル\"]] = [True, temp_rw_level]\n",
    "\n",
    "    editedlbl_lst, whole_lbls, edited_df = self.__many_level_operator(copy_labels_df, test_label,\n",
    "                                                                        temp_rw_level)\n",
    "\n",
    "    if not editedlbl_lst:\n",
    "        copy_labels_df = copy_labels_df.drop(i)\n",
    "        self.lvlarrng_log.warning(\"「%s」はテストラベルは紐づけ出来ません。\", test_label)\n",
    "        continue\n",
    "    else:\n",
    "        copy_labels_df.loc[i, [\"適用後\"]] = editedlbl_lst\n",
    "        copy_labels_df.loc[i, [\"temp_testlbl\"]] = whole_lbls\n",
    "        # copy_labels_df = pd.concat([copy_labels_df, edited_df], ignore_index=True)\n",
    "        copy_labels_df = pd.concat([copy_labels_df, edited_df])\n",
    "        self.lvlarrng_log.info(\"「%s」テストラベルの「%s」はテストラベルリストは紐づけ出来ます。\", test_label, editedlbl_lst)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "app_ini = configparser.ConfigParser()\n",
    "app_ini.read(\"./module/app.ini\", encoding='utf-8')\n",
    "spec_unit = app_ini['RequiredSpec']['spec_units']\n",
    "unitmultipliers = {u_.strip().split(\":\")[0]:int(u_.strip().split(\":\")[1]) for u_ in spec_unit.split(\",\")}\n",
    "unitmultipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import pandas as pd\n",
    "# import defect_main as dfs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def show_progress(file_name, chunk_size=1024):\n",
    "    fh = open(file_name, \"r\", encoding='cp932', errors='ignore')\n",
    "    total_size = os.path.getsize(file_name)\n",
    "    total_read = 0\n",
    "    while True:\n",
    "        chunk = fh.read(chunk_size)\n",
    "        if not chunk: \n",
    "            fh.close()\n",
    "            break\n",
    "        total_read += len(chunk)\n",
    "        print (\"Progress: %s percent\" % (total_size-total_read))\n",
    "        yield chunk\n",
    "\n",
    "for chunk in show_progress(r\"C:\\Users\\SPPC-186\\Desktop\\テスト\\Test_Files\\input\\IMX510_S撮測定要求仕様書_第3版(ES第3版)_縦筋改善レジスタ測定用_test.xlsm\"):\n",
    "    print(chunk)\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tt = {\"A\":{\"k\":1,\"d\":pd.DataFrame({\"A\":[6],\"B1\":[9]})},\"B\":{\"k\":0,\"d\":pd.DataFrame({\"A1\":[66],\"B\":[69]})}}\n",
    "\n",
    "for _, v in tt.items():\n",
    "    for k1, v1 in v.items():\n",
    "        if k1 == \"d\":\n",
    "            if \"B\" in  list(v1.columns):\n",
    "                print(v1[\"B\"].iloc[0])\n",
    "\n",
    "\n",
    "# float(\"None\")\n",
    "s = \"true\"\n",
    "s.capitalize() == \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xlsxwriter.utility import xl_range\n",
    "\n",
    "# def updateSheet(sheet_name, row, col, df_):\n",
    "#     if sheet_name not in writer.sheets:\n",
    "#         worksheet = writer.sheets[sheet_name]\n",
    "#     border_fmt = workbook.add_format({'bottom':1, 'top':1, 'left':1, 'right':1})\n",
    "#     worksheet.conditional_format(xl_range(row, col, row+len(df_), col+len(df_.columns)) , { 'type' : 'no_blanks' , 'format' : border_fmt})\n",
    "#     return worksheet\n",
    "\n",
    "writer = pd.ExcelWriter(\"test.xlsx\", engine='xlsxwriter')\n",
    "# workbook = writer.book\n",
    "border_fmt = writer.book.add_format({'bottom':1, 'top':1, 'left':1, 'right':1})\n",
    "\n",
    "df = pd.DataFrame([[38.0, 2.0, 18.0, 22.0, 21, np.nan],[19, 439, 6, 452, 226,232]],\n",
    "                  columns=pd.MultiIndex.from_product([['Decision Tree', 'Regression', 'Random'],['Tumour', 'Non-Tumour']]))\n",
    "df1 = pd.DataFrame({\"AAAAAAAA\":[1,2,3,4,5],\"BBBBBBBBBBB\":[0,9,8,7,6]})\n",
    "df2 = pd.DataFrame({\"CCCCCCCC\":[1,2,3],\"DDDDDDDDD\":[0,9,8]})\n",
    "\n",
    "row = 1\n",
    "\n",
    "df.to_excel(writer, sheet_name='Sheet1', startrow=row, startcol=1)\n",
    "worksheet = writer.sheets['Sheet1']\n",
    "worksheet.conditional_format(xl_range(row, 1, row+len(df)+2, 1+len(df.columns)) , { 'type' : 'no_errors' , 'format' : border_fmt})\n",
    "\n",
    "row += len(df)+5\n",
    "\n",
    "df1.to_excel(writer, sheet_name='Sheet1', startrow=row, startcol=5, index=False)\n",
    "# updateSheet(\"Sheet1\", row, 5, df1)\n",
    "# worksheet = writer.sheets['Sheet1']\n",
    "worksheet.conditional_format(xl_range(row+1, 5, row+len(df1)+1, 5+len(df1.columns)) , { 'type' : 'no_blanks' , 'format' : border_fmt})\n",
    "# worksheet.conditional_format(xlsxwriter.utility.xl_range(0, 0, row, 15) , { 'type' : 'no_blanks' , 'format' : border_fmt})\n",
    "row += len(df1)+4\n",
    "\n",
    "\n",
    "df2.to_excel(writer, sheet_name='Sheet1', startrow=row, startcol=5, index=False)\n",
    "border_fmt1 = writer.book.add_format({'bottom':1, 'top':0, 'left':0, 'right':1})\n",
    "worksheet.conditional_format(xl_range(row, 5, row+len(df2), 5+len(df2.columns)) , { 'type' : 'no_blanks' , 'format' : border_fmt1})\n",
    "\n",
    "\n",
    "# worksheet.conditional_format(xlsxwriter.utility.xl_range(0, 0, len(df), len(df.columns)), {'type': 'no_errors', 'format': border_fmt})\n",
    "# worksheet.conditional_format( 'A1:D12' , { 'type' : 'no_blanks' , 'format' : border_fmt})\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"A\":[\"\",\"\",\"FTTT\",\"\"],\"B\":[1,2,3,4],\"C\":[\"D\",\"|D|\",\"\",\"sdfsdf\"]})\n",
    "df[\"AA\"] = df[\"A\"].apply(lambda x: \"-\" if x else x)\n",
    "df[\"AA\"] = df[\"C\"].apply(lambda x: \"D\" if x == \"|D|\" else x)\n",
    "df\n",
    "\n",
    "df = pd.DataFrame({\"A\":[\"AAAAAAAAAAAA\",\n",
    "                        \"BBBBB\",\n",
    "                        \"CC\",\n",
    "                        \"DDDDDDDDDDDDDDDDDDDDDD\",\n",
    "                        \"ffffffff\"],\n",
    "                    \"B\":[\"111\",\"2222\",\"3\",\"44\",\"5555555555\"]})\n",
    "\n",
    "            \n",
    "df[\"A\"].apply(len).max(), df[\"B\"].apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = pd.DataFrame()\n",
    "import numpy as np\n",
    "def chage(x):\n",
    "    if \"[\" not in  x:\n",
    "        pd.concat([kk, dd[dd[\"test\"].str.contains(f\"[{x}]\")]])\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "dd = pd.DataFrame({\"test\":[\"A\",\"B\",\"C[A]\",\"D[A]\",\"E[B]\"],\"t1\":[1,2,3,4,5]})\n",
    "dd.copy()[\"test\"].apply(chage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xlsxwriter.utility import xl_range\n",
    "\n",
    "row_ = 5\n",
    "# エクセルファイル作成\n",
    "writer = pd.ExcelWriter(\"test.xlsx\", engine='xlsxwriter')\n",
    "\n",
    "# ワークブック作成\n",
    "workbook = writer.book\n",
    "# 出力データフレームのボーダーフォーマット作成\n",
    "border_fmt = workbook.add_format({'bottom':1, 'top':1, 'left':1, 'right':1})\n",
    "# マージセルのフォーマット作成\n",
    "merge_format = workbook.add_format({'bold':1,'align':'center','valign':'vcenter'})\n",
    "\n",
    "df = pd.DataFrame({\"A\":[1,2,3,4],\"B\":[2,3,4,5],\"C\":[3,4,5,6],\"D\":[4,5,6,7],\"E\":[5,6,7,8]})\n",
    "df.columns = pd.MultiIndex.from_arrays([list(\"ABKKK\"),list(\"ABCDE\")])\n",
    "\n",
    "# データフレームをワークブックに追加\n",
    "df = df.dropna()\n",
    "df.to_excel(writer, sheet_name=\"test\", startrow=row_, startcol=0)\n",
    "\n",
    "# 「test」シートを取得\n",
    "worksheet  = writer.sheets['test']\n",
    "# 必要なセルのマージ\n",
    "worksheet.merge_range(row_, 1, row_+1, 1, 'A', merge_format) # 'B1:B2'\n",
    "worksheet.merge_range(row_, 2, row_+1, 2, 'B', merge_format) # 'C1:C2'\n",
    "# テーブルのボーダーフォーマット追加\n",
    "worksheet.conditional_format(xl_range(first_row=row_, first_col=1, last_row=row_+len(df)+2, last_col=len(df.columns.to_list())) , { 'type' : 'no_errors' , 'format' : border_fmt})\n",
    "\n",
    "## 出力テンプレートを管理\n",
    "temp = pd.DataFrame([\" \"] * (len(df) + 3))\n",
    "temp.to_excel(writer, sheet_name=\"test\", startrow=row_, startcol=0, index=False, header=False)\n",
    "##\n",
    "\n",
    "#エクセル保存\n",
    "writer.save()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''if \"row\" in v:\n",
    "        # テストラベルのHYPERLINKを貼り付ける\n",
    "        macro = '=HYPERLINK(\"#\\'理由付け\\'!A{}\",\"{}\")'.format(v[\"row\"] + 1, k)\n",
    "        if a_col_v == k:\n",
    "            sheet.Cells(i - 4, \"A\").Value = macro\n",
    "            print(f\"{str(i)}  Write to {str(i - 4)} row\")\n",
    "        if g_col_v == k:\n",
    "            sheet.Cells(i - 4, \"G\").Value = macro\n",
    "            print(f\"{str(i)}  Write to {str(i - 4)} row\")\n",
    "    else:  # \"img in v\"\n",
    "        # 感度ムラの相関グラフ画像を貼り付ける\n",
    "        cell = None\n",
    "        if a_col_v == k:\n",
    "            cell = sheet.Cells(i + 9, 6 * 4)\n",
    "            print(f\"{str(i)}  Save to {str(i + 9)} row\")\n",
    "        if g_col_v == k:\n",
    "            cell = sheet.Cells(i + 9, 6 * 5)\n",
    "            print(f\"{str(i)}  Save to {str(i + 9)} row\")\n",
    "\n",
    "        if cell:\n",
    "            sheet.Shapes.AddPicture(Filename=v[\"img\"],\n",
    "                                    LinkToFile=False,\n",
    "                                    SaveWithDocument=True,\n",
    "                                    Left=cell.Left,\n",
    "                                    Top=cell.Top,\n",
    "                                    Width=270, # 350\n",
    "                                    Height=250) # 330'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "skn_path = r\"C:\\Users\\SPPC-186\\Desktop\\テスト\\Test_Files\\correlation\\項目_IMAGE5_ES1_Wf6_SKMBPC55_T_201801152300.XLSM\"\n",
    "xls_file = pd.ExcelFile(skn_path)\n",
    "\n",
    "# DATALOG_01シートの取得\n",
    "df = pd.read_excel(xls_file, sheet_name=\"グラフ&データ\", usecols=[0,6]).dropna(how=\"all\")\n",
    "df.columns=[\"A\",\"G\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ddf = df[df[\"A\"].str.contains(\"Test\", na=False)]\n",
    "ddf[\"row\"] = np.array(ddf.index)+2\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_lastRow(self, wsheet, col:int, s_row:int) -> int:\n",
    "        \"\"\"ワークシートの最後の行を抽出\n",
    "\n",
    "        Args:\n",
    "            ws (openpyxl.Workbook.Worksheet): ワークシート\n",
    "            col (int): 最後の行を取得される列\n",
    "            s_row (int): 最初の行\n",
    "\n",
    "        Returns:\n",
    "            int: 最後の行\n",
    "        \"\"\"\n",
    "        count = s_row-1\n",
    "        for cl in wsheet.iter_cols(min_col=col, max_col=col, min_row=s_row):\n",
    "            for cell in cl:\n",
    "                if not cell.value:\n",
    "                    break\n",
    "                count+=1\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse\n",
    "# inewdf_ = newdf[~newdf.isin(newdf_)].dropna(how='all') # 立上\n",
    "# iolddf_ = olddf[~olddf.isin(olddf_)].dropna(how='all') # 基準\n",
    "# return iolddf_, inewdf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __remove_maskLabels(self, labels_df):\n",
    "    \"\"\"マスク処理。マスクテストラベルから同じ座標の行削除\n",
    "\n",
    "    Args:\n",
    "        labels_df (pd.DataFrame): テストラベルセット\n",
    "    \"\"\"\n",
    "    labels_df = labels_df[labels_df[\"Sokan_indx\"] != -1]\n",
    "    allmskLabel_df = self._stdSpc_df[self._stdSpc_df[\"mask\"] != \"-\"][[\"テストラベル\",\"mask\"]]\n",
    "    print(len(allmskLabel_df))\n",
    "    mskLabel_df = allmskLabel_df.merge(labels_df, on=\"テストラベル\", how=\"inner\")\n",
    "    print(len(mskLabel_df))\n",
    "    mskLabel_df = self._defect_df.merge(mskLabel_df, on=\"テストラベル\", how=\"inner\")\n",
    "    # mskLabel_df = self._defect_df.merge(mskLabel_df, on=\"テストラベル\", how=\"inner\")\n",
    "    print(len(mskLabel_df))\n",
    "    \n",
    "    for i in range(len(mskLabel_df)):\n",
    "        df_line = mskLabel_df.iloc[i]\n",
    "        defect_label = df_line[\"Defectラベル\"]\n",
    "        mask_lst = df_line[\"mask\"].split(\",\")\n",
    "\n",
    "        wafer_list = self.__old_df[\"chip\"].drop_duplicates().to_list()\n",
    "\n",
    "        for mskL in mask_lst:\n",
    "            mskDefectL = self._defect_df[self._defect_df[\"テストラベル\"]==mskL][\"Defectラベル\"]\n",
    "            if not mskDefectL.empty:\n",
    "                mskDefectL=mskDefectL.iloc[0]\n",
    "                \n",
    "                # mask_data = []\n",
    "                for wf_no in wafer_list:\n",
    "                    # 基準\n",
    "                    temp_defectDf = self.__old_df[(self.__old_df[\"chip\"] == wf_no) & (self.__old_df[\"defect\"] == defect_label)]\n",
    "                    if not temp_defectDf.empty: \n",
    "                        temp_mskDf = self.__old_df[(self.__old_df[\"chip\"] == wf_no) & (self.__old_df[\"defect\"] == mskDefectL)]\n",
    "                        del_df = temp_defectDf.merge(temp_mskDf, on=[\"x\",\"y\"], how=\"inner\")\n",
    "                        self.__old_df = self.__old_df[(~self.__old_df.x.isin(del_df.x))&(~self.__old_df.y.isin(del_df.y))] ########\n",
    "                        print(f\"基準 {wf_no} {defect_label} {mskDefectL} : {str(len(del_df))}\")\n",
    "                    \n",
    "                    # 立上\n",
    "                    temp_defectDf = self.__new_df[(self.__new_df[\"chip\"] == wf_no) & (self.__new_df[\"defect\"] == defect_label)]\n",
    "                    if not temp_defectDf.empty:\n",
    "                        temp_mskDf = self.__new_df[(self.__new_df[\"chip\"] == wf_no) & (self.__new_df[\"defect\"] == mskDefectL)]\n",
    "                        del_df = temp_defectDf.merge(temp_mskDf, on=[\"x\",\"y\"], how=\"inner\")\n",
    "                        self.__new_df = self.__new_df[(~self.__new_df.x.isin(del_df.x))&(~self.__new_df.y.isin(del_df.y))] ########\n",
    "                        print(f\"立上 {wf_no} {defect_label} {mskDefectL} : {str(len(del_df))}\")\n",
    "                    \n",
    "                    # x_list = self.__old_df[(self.__old_df[\"chip\"] == wf_no) & (self.__old_df[\"defect\"] == defect_label)][\"x\"].to_list()\n",
    "                    # y_list = self.__old_df[(self.__old_df[\"chip\"] == wf_no) & (self.__old_df[\"defect\"] == defect_label)][\"y\"].to_list()\n",
    "                    # print(x_list, y_list)\n",
    "                    # print(len(x_list))\n",
    "                    # #wafer、テストラベル単位で座標検索一致するものあれば格納\n",
    "                    # for i in range(0, len(x_list)):\n",
    "                    #     mask = self.__old_df[(self.__old_df[\"chip\"] == wf_no) & (self.__old_df[\"defect\"] == msk) & (self.__old_df[\"x\"] == x_list[i]) & (self.__old_df[\"y\"] == y_list[i])]\n",
    "\n",
    "                    #     if not mask.empty:\n",
    "                    #         mask_data.append(mask.index[0])\n",
    "            else:\n",
    "                # need to apply image part\n",
    "                print(f\"{mskL} テストラベルが存在しません。\")\n",
    "    \n",
    "    print(\"マスク処理完了しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_convetor(levellst:str) -> str:\n",
    "    \"\"\"スライスレベルはALLの場合、一個にまとめる\n",
    "\n",
    "    Args:\n",
    "        lvlList (str): テストラベルリスト\n",
    "\n",
    "    Returns:\n",
    "        str: まとめたスライスレベル\n",
    "    \"\"\"\n",
    "    left, right = [],[]\n",
    "    d_mrk = [\"|D|\",\"D\"]\n",
    "    final_con = [\"\",\"≦\",\"\",\"＜\",\"\"]\n",
    "\n",
    "    try:\n",
    "        levelsplit = re.split(\",|、\",levellst)\n",
    "        if len(levelsplit)>1:\n",
    "            for lvl in levelsplit:\n",
    "                d_indx = 0\n",
    "                for d in d_mrk:\n",
    "                    if d in lvl:\n",
    "                        d_indx = lvl.index(d) \n",
    "                        final_con[2]= d \n",
    "                        break\n",
    "\n",
    "                con1 = re.findall(r'≦|＜|<', lvl)\n",
    "                temp_l1 = re.split('|'.join(con1), lvl)\n",
    "                con2 = re.findall(r'＞|≧|>', lvl)\n",
    "                temp_l2 = re.split('|'.join(con2), lvl)\n",
    "\n",
    "\n",
    "                if con1:                        # ≦|＜\n",
    "                    if len(temp_l1) == 2:             # one condition\n",
    "                        if d_indx == 0:\n",
    "                            right.append(temp_l1[1])\n",
    "                        else:\n",
    "                            left.append(temp_l1[0])\n",
    "                    else:                             # two condition\n",
    "                        left.append(temp_l1[0])\n",
    "                        right.append(temp_l1[2])\n",
    "                else:                           # ＞|≧\n",
    "                    if len(temp_l2) == 2:             # one condition\n",
    "                        if d_indx == 0:\n",
    "                            left.append(temp_l2[1])\n",
    "                        else:\n",
    "                            right.append(temp_l2[0])\n",
    "                    else:                             # two condition\n",
    "                        right.append(temp_l2[0])\n",
    "                        left.append(temp_l2[2])\n",
    "                        \n",
    "                        \n",
    "            left = [float(i) for i in left]\n",
    "            right = [float(i) for i in right]\n",
    "            final_con[0] = str(min(left))\n",
    "            final_con[-1] = str(max(right))\n",
    "            \n",
    "            if float(final_con[0]) < float(final_con[-1]):\n",
    "                sliceLvl = ''.join(final_con)\n",
    "            elif float(final_con[0]) == float(final_con[-1]):\n",
    "                sliceLvl = f\"D={final_con[0]}\"\n",
    "            else:\n",
    "                sliceLvl = \"-\"\n",
    "                print(f\"{str(levellst)}スライスレベルが正しくありません。\")\n",
    "        else:\n",
    "            sliceLvl = levellst\n",
    "\n",
    "    except Exception as e:\n",
    "        if re.findall(\"[<＜≦＞≧>]\",levellst):\n",
    "            sliceLvl = \"-\"\n",
    "        else:\n",
    "            sliceLvl = levellst\n",
    "\n",
    "    return sliceLvl\n",
    "\n",
    "\n",
    "# image_convetor(\"D<17,823<D\") # 823<D<17\n",
    "# image_convetor(\"100>50>D>17,823>D\") # 823<D<17\n",
    "# image_convetor(\"100>D>17,823>D\") # 823<D<17\n",
    "# image_convetor(\"teset,tttD\") # 823<D<17\n",
    "image_convetor(\"100>D\") # 100>D>17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __outputGenerator(self, olddf, newdf, tlabel, defectLabel) -> list:\n",
    "        \"\"\"出力結果DataFrame生成\n",
    "\n",
    "        Args:\n",
    "            olddf (pd.DataFrame): 基準データ\n",
    "            newdf (pd.DataFrame): 立上データ\n",
    "            test_label (str): テストラベル\n",
    "            level (str): スライスレベル\n",
    "            defectLabel (str):ディフェクトラベル」\n",
    "\n",
    "        Returns:\n",
    "            list: [要求仕様書情報、Defect連動サマリー、レベル相関情報、Defect連動情報、Topic値]:出力DataFrameリスト\n",
    "        \"\"\"\n",
    "\n",
    "        # 測定要求仕様書情報\n",
    "        specDetails_cols = [\"大分類\", \"小分類\", \"テストラベル\", \"スライスレベル\", \"Unit\"]\n",
    "        specDetails_df = self._stdSpc_df[self._stdSpc_df[\"テストラベル\"] == tlabel][specDetails_cols]\n",
    "        specDetails_df.insert(loc=2, column=\"Defectラベル\", value=[defectLabel])\n",
    "\n",
    "        # レベル相関情報\n",
    "        detailsSokan_cols = [\"相関係数\", \"オフセット\"]\n",
    "        detailsSokan_data = [[np.nan, np.nan]]  # need to calculate ★\n",
    "        detailsSokan_df = pd.DataFrame(detailsSokan_data, columns=detailsSokan_cols)\n",
    "\n",
    "        old_Df, new_Df = olddf, newdf\n",
    "        old_Df[\"tempx\"] = old_Df[\"x\"]\n",
    "        new_Df[\"tempx\"] = new_Df[\"x\"]\n",
    "        old_Df[\"tempy\"] = old_Df[\"y\"]\n",
    "        new_Df[\"tempy\"] = new_Df[\"y\"]\n",
    "\n",
    "        mrg_df = pd.merge(old_Df, new_Df, on=['chip', 'defect', 'tempx', \"tempy\"], how='outer')\n",
    "        mrg_df = mrg_df.sort_values(by=['chip', 'defect', 'tempx', \"tempy\"])\n",
    "\n",
    "        # Defect連動サマリー\n",
    "        ## スライスレベル近辺\n",
    "        nearVal = \"無し\"\n",
    "        if len(mrg_df):\n",
    "            nearVal = \"有り\"\n",
    "\n",
    "        ## アドレス差分\n",
    "        difference = \"有り\"\n",
    "        mrg_df[\"差分\"] = mrg_df.apply(lambda x: True if x['x_x'] == x['x_y'] and x['y_x'] == x['y_y'] else False, axis=1)\n",
    "        if True in mrg_df[\"差分\"].values:\n",
    "            difference = \"無し\"\n",
    "\n",
    "        detailsSummry_cols = [\"アドレス差分\", \"スライスレベル近辺\"]\n",
    "        detailsSummry_data = [[difference, nearVal]]\n",
    "        detailsSummry_df = pd.DataFrame(detailsSummry_data, columns=detailsSummry_cols)\n",
    "        mrg_df = mrg_df.drop(columns=[\"tempx\", \"tempy\", \"差分\"])\n",
    "\n",
    "        # Defect連動情報\n",
    "        ## チップ情報\n",
    "        # chip_cols = [\"Wafer(X-Y)\",\"チップNo\",\"Defectラベル\"]\n",
    "        chip_cols = [\"Wafer(X-Y)\", \"チップNo\"]\n",
    "        chipData_df = pd.DataFrame(columns=chip_cols)\n",
    "        chipData_df[\"Wafer(X-Y)\"] = mrg_df[\"chip\"].apply(lambda x: int(x / 10000))\n",
    "        chipData_df[\"チップNo\"] = mrg_df[\"chip\"].apply(lambda x: int(x % 10000))\n",
    "        ### マルチヘッダー追加\n",
    "        ctopicCols = [(\"\", ctopic) for ctopic in chip_cols]\n",
    "        chipData_df.columns = pd.MultiIndex.from_tuples(ctopicCols)\n",
    "\n",
    "        ## Defectデータ情報\n",
    "        defectData_df = mrg_df.drop(columns=[\"chip\"])\n",
    "        defectData_df.columns = [\"defect\"] + self.__DEFECTDATA_COLS[0:4] + self.__DEFECTDATA_COLS[8:12]\n",
    "\n",
    "        ### X軸再測、Y軸再測のdummy作成\n",
    "        defectData_df[self.__DEFECTDATA_COLS[4:8] + self.__DEFECTDATA_COLS[12:]] = pd.DataFrame(\n",
    "            [[np.nan] * 8] * len(mrg_df), index=mrg_df.index)\n",
    "        ### 列のインデックスを再作成\n",
    "        defectData_df = defectData_df.reindex(columns=[\"defect\"] + self.__DEFECTDATA_COLS)\n",
    "        ### マルチヘッダー追加\n",
    "        topicCol_lst = [(mtopic, self.__DEFECTDATA_COLS[i * 4], self.__DEFECTDATA_COLS[(i * 4) + 3]) for mtopic, i in\n",
    "                        zip(self.__RESULT_COLS, range(0, int(len(self.__DEFECTDATA_COLS) / 4)))]\n",
    "        dtopicCols = [(\"\", \"Defectラベル\")] + [(new, c) for new, start, end in topicCol_lst for c in\n",
    "                                            defectData_df.loc[:, start:end].columns]\n",
    "        defectData_df.columns = pd.MultiIndex.from_tuples(dtopicCols)\n",
    "\n",
    "        return [specDetails_df, detailsSummry_df, detailsSokan_df, chipData_df, defectData_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # inputs\n",
    "spec = r\"C:\\Users\\SPPC-186\\Desktop\\Test_Files\\input\\IMX510_S撮測定要求仕様書_第3版(ES第3版)_縦筋改善レジスタ測定用.xlsm\"\n",
    "spec1 = r\"C:\\Users\\SPPC-186\\Desktop\\Test_Files\\input\\規格シートの最新版サンプル.xlsm\"\n",
    "old_Dpath = r\"C:\\Users\\SPPC-186\\Desktop\\Test_Files\\correlation\\ID510AQP1_ESD510E1R02_ESD510E1R-06.D\"\n",
    "new_Dpath = r\"C:\\Users\\SPPC-186\\Desktop\\Test_Files\\correlation\\ID510AQP1_ESD510E3R01_ESD510E3R-06.D\"\n",
    "sokan = r\"C:\\Users\\SPPC-186\\Desktop\\Test_Files\\correlation\"\n",
    "\n",
    "yokyu = openpyxl.load_workbook(spec) \n",
    "# dfss = dfs.Defect_sys()\n",
    "\n",
    "# return_df = dfss.systemData_collector(spec, old_Dpath, new_Dpath, sokan)\n",
    "# return_df\n",
    "# liceData_df = dfss.systemData_collector(spec, old_Dpath, new_Dpath, sokan)\n",
    "# liceData_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = yokyu[\"規格（撮像）\"] # 「仕様表紙」シート\n",
    "book_ver = re.split(\"[._]\", ws['A1'].value)[1]\n",
    "book_ver\n",
    "df = pd.DataFrame(ws.values)#[\"B7\":\"AA2479\"]\n",
    "extract_col = [1, 2, 3, 4, 6, 7, 8, 9, 10, 13, 18, 26, 29, 30, 31]\n",
    "col_names = [\"大分類\", \"中分類\", \"小分類\", \"テストラベル\", \"l-limit\", \"h-limit\", \"spec-unit\", \"スライスレベル\", \"Unit\", \"パラメータ/式\",\n",
    "                                \"image\", \"mask\", \"LSB換算名\", \"LSB諧調\", \"LSBGain\"]\n",
    "\n",
    "df = df.iloc[6:2479][extract_col]\n",
    "df.columns=col_names\n",
    "df= df.fillna(\"-\")\n",
    "for i,v in df.items():\n",
    "    if len(v) < 1:\n",
    "        print(i,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob,os\n",
    "from itertools import chain\n",
    "\n",
    "def findAllDir(path_:str):\n",
    "    if os.path.isfile(path_):\n",
    "        return path_ if path_.lower().endswith(\".xlsm\") else None\n",
    "    else:\n",
    "        lst = list(filter(None, [findAllDir(subPath) for subPath in glob.glob(path_+\"/*\")]))\n",
    "        return lst  \n",
    "\n",
    "# findAllDir(r\"C:\\Users\\SPPC-186\\Desktop\\Test_Files\")\n",
    "# regular_list = findAllDir(r\"C:\\Users\\SPPC-186\\Desktop\\Test_Files\")\n",
    "p = \"C:\\\\Users\\\\SPPC-186\\\\Desktop\\\\Test_Files\\\\correlation\\\\理由付けアウトプット\\\\項目_IMAGE5_ES1_Wf6_SKMBPC55_T_201801152300.XLSM\"\n",
    "p.lower().endswith(\".xlsm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# return_df[\"しきい値\"]=5\n",
    "# return_df = dfss.testLevel_operator(return_df)\n",
    "# return_df\n",
    "# print(ValueError(\"無効な条件\"))\n",
    "# t = \"1\"\n",
    "# re.search(\"[a-zA-Z]\",t)\n",
    "# not isinstance(1, str)\n",
    "os.path.splitext(\"C:/Users/SPPC-186/Desktop/Test_Files/項目_IMAGE5_ES1_Wf6_SKMBPC55_T_201801154200.XLSM\")\n",
    "r\"C:\\Users\\SPPC-186\\Desktop\\Test_Files\\項目_IMAGE5_ES1_Wf6_SKMBPC55_T_201801154200.XLSM\".rsplit(\".\",1)\n",
    "datetime.date.fromtimestamp(int(os.path.getmtime(\"C:/Users/SPPC-186/Desktop/Test_Files/項目_IMAGE5_ES1_Wf6_SKMBPC55_T_201801154200.XLSM\"))).strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "old_Dpath = r\"C:\\Users\\SPPC-186\\Desktop\\Test_Files\\correlation\\ID510AQP1_ESD510E1R02_ESD510E1R-06.D\"\n",
    "new_Dpath = r\"C:\\Users\\SPPC-186\\Desktop\\Test_Files\\correlation\\ID510AQP1_ESD510E3R01_ESD510E3R-06.D\"\n",
    "def dfile(path):\n",
    "    dfile_df = pd.read_csv(path, sep=\" \")\n",
    "    dfile_df.columns=[\"chip\", \"defect\" , \"unit\" , \"x\" , \"y\" , \"result\"]\n",
    "    return dfile_df\n",
    "old_df_ = dfile(old_Dpath)\n",
    "new_df_ = dfile(new_Dpath)\n",
    "\n",
    "dLabel = \"DGS1_\"\n",
    "unit = \"mv\"\n",
    "oldDf = old_df_[(old_df_[\"defect\"]==dLabel)].iloc[:100]\n",
    "newDf = new_df_[(new_df_[\"defect\"]==dLabel)].iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_Df, new_Df = oldDf, newDf\n",
    "\n",
    "old_Df = old_Df.drop(columns=[\"defect\", \"unit\"])\n",
    "new_Df = new_Df.drop(columns=[\"defect\", \"unit\"])\n",
    "\n",
    "mrg_df = pd.merge(old_Df, new_Df, on=['chip', 'x', \"y\"], how='outer')\n",
    "mrg_df = mrg_df.sort_values(by=['chip','x', \"y\"])\n",
    "mrg_df = mrg_df.fillna(\"\")\n",
    "\n",
    "topic = f\"【Defectラベル】{dLabel}【Unit】{unit}\"\n",
    "mrg_df = mrg_df.reindex(columns=['chip', 'x', \"y\", \"result_x\", \"result_y\"])\n",
    "mrg_df.columns = [\"chip\", \"Xaddr\", \"Yaddr\", \"X軸\", \"Y軸\"]\n",
    "\n",
    "if not mrg_df.empty:\n",
    "    mrg_df[\"分類\"] = mrg_df.apply(lambda x: \"\" if len(str(x['X軸'])) and len(str(x['Y軸'])) else \"▲\", axis=1)\n",
    "else:\n",
    "    mrg_df[\"分類\"] = []\n",
    "\n",
    "# mrg_df1 = mrg_df[:1]\n",
    "# m1 = mrg_df\n",
    "# m2 = mrg_df.iloc[1][mrg_df.columns[:3]]\n",
    "# edited_df = pd.concat([m1, m2], ignore_index=True)\n",
    "# edited_df\n",
    "m1 = mrg_df[mrg_df[\"Xaddr\"]==17][mrg_df.columns[3:]]\n",
    "m1[\"chip\"] = [5]\n",
    "m2 = mrg_df[mrg_df[\"Xaddr\"]==36]\n",
    "\n",
    "edited_df = pd.concat([m2, m1], ignore_index=True)\n",
    "edited_df.reindex(columns=mrg_df.columns)\n",
    "# t1 = [\"A\",\"B\"]\n",
    "# t2 = [\"C\", \"A\"]\n",
    "m11 = edited_df.drop(columns=[\"chip\"])\n",
    "m12 = edited_df.drop(columns=[\"分類\"])\n",
    "\n",
    "edited_df\n",
    "# tt = tupleToconvt(t1)+ignoreLabel(t1, t2)\n",
    "# for i, (k, n) in enumerate(tt):\n",
    "#     print(i,k,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "d = pd.DataFrame({\"A\":[1,2,3,4,np.nan],\"B\":[5,6, np.nan,7,8]})\n",
    "d1 = pd.DataFrame(columns=[\"A\",\"B\"])\n",
    "d.fillna(\"-\")\n",
    "\n",
    "from openpyxl.formula.translate import Translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def testlvl_convetor(lvl_arg):\n",
    "        \"\"\"仕様書のスライスレベル変換\n",
    "\n",
    "        Args:\n",
    "            lvl_arg (str): スライスレベル\n",
    "\n",
    "        Returns:\n",
    "            str: テストラベル\n",
    "            lst: ALLのテストラベルセット\n",
    "        \"\"\"\n",
    "\n",
    "        level_all = \",|、\"\n",
    "        level_lst = {\"=\":\"==\",\n",
    "                    \"＜\":\"<\",\n",
    "                    \"≦\":\"<=\",\n",
    "                    \"＞\":\">\",\n",
    "                    \"≧\":\">=\",\n",
    "                    \"|D|\":\"abs(D)\"}\n",
    "\n",
    "        lable_lst = []\n",
    "        \n",
    "        if not re.search(level_all, lvl_arg):\n",
    "            # 条件に範囲追加\n",
    "            num_lst = re.findall(\"\\d+\\.\\d+\", lvl_arg)\n",
    "            for nl in num_lst:\n",
    "                temp = float(nl)*1000 # 1000を掛ける\n",
    "                lvl_arg = lvl_arg.replace(nl, str(temp))\n",
    "\n",
    "            for lvl in level_lst:\n",
    "                if lvl in lvl_arg:\n",
    "                    lvl_arg = lvl_arg.replace(lvl, level_lst[lvl])\n",
    "        else:\n",
    "            lable_lst = re.split(level_all, lvl_arg)\n",
    "            \n",
    "        return lvl_arg,lable_lst\n",
    "\n",
    "testlvl_convetor(\"abcD,askjakds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specDetails_df = pd.DataFrame({\"\":[\"X軸\",\"Y軸\"],\n",
    "                               \"大分類\":[\"DK Glob\",\"DK Glob\"],\n",
    "                               \"小分類\":[\"個数\",\"個数\"],\n",
    "                               \"分類\":[\"対像欠陥\", \"対像欠陥\"],\n",
    "                               \"テストラベル\":[\"DGS1_ZIC\",\"DGS1_ZIC\"],\n",
    "                               \"スライスレベル\":[\"0.48＜D\",\"0.48＜D\"],\n",
    "                               \"Unit\":[\"mv\",\"mv\"],\n",
    "                               \"LSB諧調\":[\"0.0:\",\"0.0:\"],\n",
    "                               \"Defectラベル\":[\"DGS1_\",\"DGS1_\"]})\n",
    "detailsSummry_df = pd.DataFrame({\"抽出閾値\":[\"**\",\"**\"],\"測定LSB諧調\":[\"**\",\"**\"]})\n",
    "detailsSokan_df = pd.DataFrame({\"組閣関係\":[\"\"],\"オフセット\":[\"\"]})\n",
    "\n",
    "chip_cols = [\"Wafer(X-Y)\",\"チップNo.\",\"Xaddr\",\"Yaddr\"]\n",
    "chipData_df = pd.DataFrame([[6,1,200,500],\n",
    "                            [6,1,203,600],\n",
    "                            [6,1,205,650],\n",
    "                            [6,1,207,700]], columns=chip_cols)\n",
    "ctopicCols = [(\"\", ctopic) for ctopic in chip_cols]\n",
    "chipData_df.columns = pd.MultiIndex.from_tuples(ctopicCols)\n",
    "\n",
    "dfct_cols = [\"X軸\", \"X軸再測\", \"Y軸\", \"Y軸再測\", \"分類\"]\n",
    "defectData_df = pd.DataFrame([[\"502\",\"502\",\"\",\"\",\"▲\"],\n",
    "                              [\"\",\"\",\"302\",\"450\",\"▲\"],\n",
    "                              [\"509\",\"508\",\"\",\"\",\"▲\"],\n",
    "                              [\"\",\"\",\"390\",\"379\",\"▲\"]])\n",
    "dtopicCols = [(\"\", dtopic) if dtopic == dfct_cols[-1] else (\"【Defectラベル】DGS1_ 【Unit】mv\", dtopic) for dtopic in dfct_cols]\n",
    "defectData_df.columns = pd.MultiIndex.from_tuples(dtopicCols)\n",
    "def style_negative(v, props=''):\n",
    "    return None if v == \"\" else props\n",
    "defectData_df.style.applymap(style_negative, props='background-color:yellow;')\n",
    "defectData_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "weather_df = pd.DataFrame(np.random.rand(10,2)*5,\n",
    "                          index=pd.date_range(start=\"2021-01-01\", periods=10),\n",
    "                          columns=[\"Tokyo\", \"Beijing\"])\n",
    "weather_df\n",
    "s = weather_df.style.format('{:.0f}').hide([('Random', 'Tumour'), ('Random', 'Non-Tumour')], axis=\"columns\")\n",
    "s.set_table_styles([  # create internal CSS classes\n",
    "    {'selector': '.true', 'props': 'background-color: #e6ffe6;'},\n",
    "    {'selector': '.false', 'props': 'background-color: #ffe6e6;'},\n",
    "], overwrite=False)\n",
    "cell_color = pd.DataFrame([['true ', 'false ', 'true ', 'false '],\n",
    "                           ['false ', 'true ', 'false ', 'true ']],\n",
    "                          index=weather_df.index,\n",
    "                          columns=weather_df.columns[:4])\n",
    "s.set_td_classes(cell_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "writer = pd.ExcelWriter(\"temp_理由付け.xlsx\", engine='xlsxwriter')\n",
    "temp_wb = writer.book\n",
    "temp_ws = temp_wb.add_worksheet(\"理由付け\")\n",
    "writer.sheets[\"理由付け\"] = temp_ws\n",
    "\n",
    "def output(outputData_lst, row_):\n",
    "    # outputData_lst = pd.DataFrame()\n",
    "    # specDetails_df, detailsSummry_df, detailsSokan_df, chipData_df, defectData_df = outputData_lst\n",
    "\n",
    "    header_lst = [specDetails_df[\"テストラベル\"].iloc[0], \"◆測定要求仕様書情報\", \"◆抽出定義\", \"◆レベル相関情報\", \"◆Defect連動情報\"]\n",
    "\n",
    "    # テストラベルヘッダと相関グラフへ戻るHyperlink書く\n",
    "    temp_df = pd.DataFrame([header_lst[0], '=HYPERLINK(\"#\\'グラフ&データ\\'!A1\",\"相関グラフへ戻る\")'])\n",
    "    temp_df.to_excel(writer, sheet_name=\"理由付け\", startrow=row_, startcol=0, index=False, header=False)\n",
    "    row_ += 3\n",
    "\n",
    "    # 測定要求仕様書情報\n",
    "    ## 【測定要求仕様書情報】ヘッダ書く\n",
    "    temp_df = pd.DataFrame([header_lst[1]])\n",
    "    temp_df.to_excel(writer, sheet_name=\"理由付け\", startrow=row_, startcol=1, index=False, header=False)\n",
    "    row_ += 1\n",
    "\n",
    "    ## 要求仕様書情報書く\n",
    "    specDetails_df.to_excel(writer, sheet_name=\"理由付け\", startrow=row_, startcol=1, index=False)\n",
    "    # row_ += len(specDetails_df) + 3\n",
    "\n",
    "    # 抽出定義\n",
    "    ## 【抽出定義】ヘッダ書く\n",
    "    temp_df = pd.DataFrame([header_lst[2]])\n",
    "    temp_df.to_excel(writer, sheet_name=\"理由付け\", startrow=row_-1, startcol=11, index=False, header=False)\n",
    "    # row_ += 1\n",
    "\n",
    "    ## 抽出定義書く\n",
    "    detailsSummry_df.to_excel(writer, sheet_name=\"理由付け\", startrow=row_, startcol=11, index=False)\n",
    "    # row_ += len(detailsSummry_df) + 3\n",
    "\n",
    "    # レベル相関情報\n",
    "    ## 【レベル相関情報】ヘッダ書く\n",
    "    temp_df = pd.DataFrame([header_lst[3]])\n",
    "    temp_df.to_excel(writer, sheet_name=\"理由付け\", startrow=row_-1, startcol=14, index=False, header=False)\n",
    "    # row_ += 1\n",
    "\n",
    "    ## レベル相関情報書く\n",
    "    detailsSokan_df.to_excel(writer, sheet_name=\"理由付け\", startrow=row_, startcol=14, index=False)\n",
    "    # row_ += len(detailsSokan_df) + 3\n",
    "    row_ += len(specDetails_df) + 5\n",
    "\n",
    "    # Defect連動情報\n",
    "    ## 【Defect連動情報】ヘッダ書く\n",
    "    temp_df = pd.DataFrame([header_lst[4]])\n",
    "    temp_df.to_excel(writer, sheet_name=\"理由付け\", startrow=row_, startcol=1, index=False, header=False)\n",
    "    row_ += 1\n",
    "\n",
    "    ## Defectデータ情報\n",
    "    defectData_df.to_excel(writer, sheet_name=\"理由付け\", startrow=row_, startcol=4)  # index=False\n",
    "\n",
    "    ## チップ情報\n",
    "    chipData_df.to_excel(writer, sheet_name=\"理由付け\", startrow=row_, startcol=0)  # index=False\n",
    "\n",
    "    ## 出力テンプレートを管理するため\n",
    "    temp = pd.DataFrame([\" \"] * (len(chipData_df) + 3))\n",
    "    temp.to_excel(writer, sheet_name=\"理由付け\", startrow=row_, startcol=0, index=False, header=False)\n",
    "    temp = pd.DataFrame([[\" \", \" \", \" \", \" \"]])\n",
    "    temp.to_excel(writer, sheet_name=\"理由付け\", startrow=row_, startcol=1, index=False, header=False)\n",
    "\n",
    "    row_ += len(defectData_df) + 4\n",
    "\n",
    "    # Defect Run情報開くHyperlink\n",
    "    temp_df = pd.DataFrame(['=HYPERLINK(\"#\\'グラフ&データ\\'!A1\",\"Defect Raw情報展開\")'])\n",
    "    temp_df.to_excel(writer, sheet_name=\"理由付け\", startrow=row_, startcol=1, index=False, header=False)\n",
    "    row_ += 2\n",
    "\n",
    "    writer.close()\n",
    "    return row_\n",
    "\n",
    "\n",
    "# output({}, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catg1 = [\"HL\", \"PHL\", \"FL\"]\n",
    "catg2_3 = [\"YCODE\",\"YOUT\",\"GCODE\",\"YLINE\",\"YNIK\",\"YLCAL\",\"YFRM\",\"YGLOB\",\"TGLOB\",\"LDENL\",\"LMAXL\",\"DLINE\",\"DLCAL\",\"DFRM\",\"DGLOB\"]\n",
    "catg2_5 = [\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"905\",\"10\",\"15\",\"20\",\"30\",\"35\",\"40\"]\n",
    "catg2_6 = [\"AV\",\"MX\",\"MN\",\"LBL\"]\n",
    "catg3 = [\"Z2\",\"Z2D\",\"ZCC\",\"ZF\",\"ZS\",\"ZK1\",\"ZK2\"]\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import process,fuzz\n",
    "\n",
    "def get_subLists(catgry :str,lst:list) -> list :\n",
    "    \"\"\"lstリストのサブパータンリスト作成\n",
    "\n",
    "    Args:\n",
    "        catgry (str): 分類名\n",
    "        lst (list): 分類のサブ分類リスト\n",
    "\n",
    "    Returns:\n",
    "        list: サブパターンリスト\n",
    "    \"\"\"\n",
    "    # label1の文字の頻度がlabel2より大きさの確認\n",
    "    compareLabels = lambda label1, label2 : not bool(Counter(label2.lower())-Counter(label1.lower()))\n",
    "\n",
    "    lists_ = []\n",
    "    for i in range(1, len(lst) + 1):\n",
    "        for j in range(i):\n",
    "            if lst[j: i] != lst[-1]:\n",
    "                templ = \"\".join(lst[j: i])\n",
    "                templ_ = \"\".join(lst[:j]+lst[i:])\n",
    "                if compareLabels(catgry ,templ) and (templ not in lists_):\n",
    "                    lists_ .append(templ)\n",
    "                if compareLabels(catgry ,templ_) and (templ_ not in lists_):\n",
    "                    lists_.append(templ_)\n",
    "    return [i for i in lists_ if lst[0] in i]\n",
    "\n",
    "def get_pattern(subCatDict:dict) -> list: \n",
    "    \"\"\"サブリストによる、テストラベルパターンセット作成\n",
    "\n",
    "    Args:\n",
    "        subCatDict (dict): サブ分類dict\n",
    "\n",
    "    Returns:\n",
    "        list: テストラベルパターンセット\n",
    "    \"\"\"\n",
    "    pLst = []\n",
    "    for val in subCatDict.values():\n",
    "        if not len(val):    \n",
    "            continue\n",
    "        \n",
    "        if len(val) != 1:\n",
    "            temp = []\n",
    "            for pe in pLst:\n",
    "                for e in val:\n",
    "                    temp.append(pe+\"_\"+e) \n",
    "\n",
    "            pLst = temp if len(pLst) else val\n",
    "        else:\n",
    "            if len(pLst):\n",
    "                temp = []\n",
    "                for pe in pLst:\n",
    "                    temp.append(pe+\"_\"+val[-1])\n",
    "                pLst = temp\n",
    "            else:\n",
    "                pLst.append(val[-1])\n",
    "    return pLst\n",
    "\n",
    "def get_kandomuraSlicLvl(testLbl:str, kmDataset:dict) -> str:\n",
    "    \"\"\"スライスレベル無し感度ムラテストラベルとマッチングするスライスレベル設定\n",
    "\n",
    "    Args:\n",
    "        testLbl (str): テストラベル\n",
    "        kmDataset (dict): 感度ムラデータセット\n",
    "\n",
    "    Returns:\n",
    "        str: スライスレベル \n",
    "    \"\"\"\n",
    "    catSubLst = {}\n",
    "    getDetails = lambda lbl, lst: [i for i in lst if i in lbl]\n",
    "    detailConfm = lambda lbl, lst: [i for i in lst if i == lbl]\n",
    "\n",
    "    # テストラベルは分類分割\n",
    "    category = testLbl.split(\"_\")\n",
    "    print(category) \n",
    "\n",
    "    snglCat = []\n",
    "\n",
    "    for cat in category:\n",
    "        for mk, mv in kmDataset.items():\n",
    "            if len(catSubLst) and mk < list(catSubLst.keys())[-1]: \n",
    "            # if mk in catSubLst: \n",
    "            # already exist in catSubLst then skip\n",
    "                continue\n",
    "\n",
    "            if isinstance(mv, dict):\n",
    "                # tempStr = cat\n",
    "                for _, sv in mv.items():\n",
    "                    temp = detailConfm(cat, sv)\n",
    "                    if len(temp):\n",
    "                        catSubLst[mk]=[cat, temp]\n",
    "                        break\n",
    "\n",
    "                    temp = getDetails(cat, sv)\n",
    "                    if len(temp):\n",
    "                        if mk in catSubLst:\n",
    "                            catSubLst[mk][-1] += temp\n",
    "                        else:\n",
    "                            catSubLst[mk] = [cat, temp]\n",
    "            else:\n",
    "                snglCat.append(mk)\n",
    "                temp = detailConfm(cat, mv)\n",
    "                if len(temp):\n",
    "                    catSubLst[mk]=temp\n",
    "            \n",
    "            if mk in catSubLst and len(catSubLst) >= category.index(cat)+1: \n",
    "            # if mk in catSubLst: \n",
    "            # current key exist in catSubLst then break\n",
    "                break\n",
    "\n",
    "    print(catSubLst)\n",
    "\n",
    "    for key, lVal in catSubLst.copy().items():\n",
    "        if not key in snglCat:\n",
    "            catSubLst[key] = get_subLists(lVal[0], lVal[-1])\n",
    "\n",
    "    print(catSubLst)\n",
    "\n",
    "    pattrn = get_pattern(catSubLst)\n",
    "    if testLbl in pattrn:\n",
    "        pattrn.remove(testLbl)\n",
    "\n",
    "    temp_category = lambda lst: [i.split(\"_\") for i in lst]\n",
    "\n",
    "\n",
    "    # ratiovals = []\n",
    "    # catLen = len(category)\n",
    "    # for ptn in temp_category(pattrn):\n",
    "    #     if catLen==len(ptn):\n",
    "    #         temprto = []\n",
    "    #         for i in range(catLen):\n",
    "    #             temprto.append(fuzz.ratio(category[i].lower(), ptn[i].lower()))\n",
    "    #         ratiovals.append(temprto)\n",
    "    \n",
    "    # print(ratiovals)\n",
    "        #     for cat in category:\n",
    "        #         ratios = process.extract(cat,strOptions)\n",
    "        # process.extractOne(slNo,lblLst)\n",
    "\n",
    "    return pattrn\n",
    "\n",
    "\n",
    "\n",
    "slNo = \"PHLBE_YFRMMX_Z2D_F\" # slice level not exist\n",
    "# slNo = \"8_YFRMMX_Z2D\" # slice level not exist\n",
    "print(slNo)\n",
    "# lblLst = get_kandomuraSlicLvl(slNo, dataset)\n",
    "lblLst = get_kandomuraSlicLvl(slNo, dataset)\n",
    "print(lblLst)\n",
    "\n",
    "# kndmura_df = spec_df[(spec_df[\"中分類\"]==\"感度ムラ\")&(spec_df[\"スライスレベル\"]!=\"-\")][[\"テストラベル\",\"スライスレベル\"]]\n",
    "# print(kndmura_df)\n",
    "# kndmuraNotSlce_df = spec_df[(spec_df[\"中分類\"]==\"感度ムラ\")&(spec_df[\"スライスレベル\"]==\"-\")][[\"テストラベル\",\"スライスレベル\"]]\n",
    "# print(kndmuraNotSlce_df)\n",
    "# kndmuraNotSlce_df = kndmuraNotSlce_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "path = r\"C:\\Users\\SPPC-186\\Desktop\\Test_Files\\input\\IMX510_S撮測定要求仕様書_第3版(ES第3版)_縦筋改善レジスタ測定用.xlsm\"\n",
    "path1 = r\"C:\\Users\\SPPC-186\\Desktop\\Test_Files\\input\\規格シートの最新版サンプル.xlsm\"\n",
    "\n",
    "scol_index = [1, 2, 3, 4, 8, 9, 10, 13, 18, 26, 29, 30, 31, 39]\n",
    "scolNames = [\"大分類\", \"中分類\", \"小分類\", \"テストラベル\", \"spec-unit\", \"スライスレベル\", \"Unit\", \"パラメータ/式\", \"image\", \"mask\", \"LSB換算名\", \"LSB諧調\", \"LSBGain\", \"Defectラベル\"]\n",
    "dcol_index = [1, 8]\n",
    "dcolNames = [\"Defectラベル\", \"テストラベル\" ]\n",
    "lsb_index = [2, 4, 5, 6, 7]\n",
    "lsbNmaes = [\"LSB換算名\", \"階調\", \"Gain\", \"unit\", \"1LSB出力\"]\n",
    "\n",
    "xlsFile = pd.ExcelFile(path)\n",
    "\n",
    "# def extract_sheetData(sheetName:str, lastrow:int=0):\n",
    "def extract_sheetData(workbook, sheetName:str):\n",
    "    spec_df = pd.DataFrame(columns=scolNames)\n",
    "    defct_df = pd.DataFrame(columns=dcolNames)\n",
    "\n",
    "    rang = range(1,6) if \"ディフェクト\" not in sheetName else range(1,4) \n",
    "    tempdf = pd.read_excel(workbook, sheet_name=sheetName, skiprows=rang)\n",
    "    version = re.split(\"[._]\",str(tempdf.columns[0]))[1]\n",
    "\n",
    "    if version == \"2\":\n",
    "        if \"ディフェクト\" in sheetName:\n",
    "            defct_df = tempdf.iloc[:, dcol_index].fillna(\"-\")\n",
    "            defct_df.columns = dcolNames\n",
    "        else:\n",
    "            spec_df = tempdf.iloc[:, scol_index[:-1]].fillna(\"-\")\n",
    "            spec_df.columns = scolNames[:-1]\n",
    "            # spec_df[[\"lastRow\"]]=[lastrow]\n",
    "    elif version == \"3\":\n",
    "        spec_df = tempdf.iloc[:, scol_index].fillna(\"-\")\n",
    "        spec_df.columns = scolNames\n",
    "        defct_df = spec_df[dcolNames]\n",
    "        spec_df = spec_df.drop(columns=scolNames[-1])\n",
    "        # spec_df[[\"lastRow\"]]=[lastrow]\n",
    "    else:\n",
    "        print(f\"{version}バージョンはシステム設定していませえん\")\n",
    "\n",
    "    defct_df = defct_df[defct_df[dcolNames[0]] != \"-\"]\n",
    "    spec_df = spec_df[spec_df[scolNames[0]] != \"-\"]\n",
    "\n",
    "    # lastrow += len(spec_df)\n",
    "    # return spec_df, defct_df, lastrow\n",
    "    return spec_df, defct_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "dd = pd.DataFrame({\"A\":[1,2,3,4,5],\"B\":[3,4,5,6,7]})\n",
    "ff = dd.iloc[[0]]\n",
    "ff[\"A\"].item()\n",
    "# pd.merge(dd,ff, how=\"inner\", on=[\"A\",\"B\"])\n",
    "list(dd.columns)\n",
    "[\"2\",\"3\"]+[]\n",
    "# t,y,u=[]\n",
    "# re.findall(\"\\w+\", re.split(\"単独点\",\"$単独点=OF_ZP90,DK_ZL2,HL_ZL2_ML\")[-1])\n",
    "pramVal = re.split(\";\",\"$検出カラーマップ=Bayer;$単独点=OF_ZP90,DK_ZL2,HL_ZL2_ML\")\n",
    "param_testLbl = [re.findall(\"\\w+\", re.split(\"単独点\",v)[-1]) for v in pramVal if \"単独点\" in v][0]\n",
    "param_testLbl\n",
    "re.split(\"0\",\"D<=731>6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "        try:\n",
    "            if bookver == \"3\" and \"ディフェクト\" in sheetName:\n",
    "                spec_df = self._stdSpc_df[self._stdSpc_df[\"Defectラベル\"] != \"-\"][[\"テストラベル\", \"Defectラベル\"]]\n",
    "                self._stdSpc_df = self._stdSpc_df.drop('Defectラベル', axis=1)\n",
    "                return spec_df, bookver\n",
    "\n",
    "            extract_col, col_names = [], []\n",
    "            ws = workbook[sheetName]\n",
    "\n",
    "            # 要求仕様書のVer取得\n",
    "            bookver = re.split(\"[._]\", ws[self.versionCell].value)[1]       \n",
    "\n",
    "            if \"規格\" in sheetName:\n",
    "                col, s_row = 2, 7\n",
    "                strt = \"{}{}\".format(xlutil.get_column_letter(col), s_row)  # \"B7\"\n",
    "                if bookver == \"2\":\n",
    "                    end = \"AN{}\".format(self.__get_lastRow(ws, col, s_row))  # \"AA2479\"\n",
    "                    extract_col = [0, 1, 2, 3, 5, 6, 7, 8, 9, 12, 17, 25, 29]\n",
    "                    col_names = [\"大分類\", \"中分類\", \"小分類\", \"テストラベル\", \"l-limit\", \"h-limit\", \"spec-unit\", \"スライスレベル\", \"Unit\", \"パラメータ/式\",\n",
    "                                \"image\", \"mask\", \"LSB諧調\"]\n",
    "\n",
    "                elif bookver == \"3\":\n",
    "                    end = \"AN{}\".format(self.__get_lastRow(ws, col, s_row))  # \"AN86\" ← in new format\n",
    "                    extract_col = [0, 1, 2, 3, 5, 6, 7, 8, 9, 12, 17, 25, 29, 38]\n",
    "                    col_names = [\"大分類\", \"中分類\", \"小分類\", \"テストラベル\", \"l-limit\", \"h-limit\", \"spec-unit\", \"スライスレベル\", \"Unit\", \"パラメータ/式\",\n",
    "                                \"image\", \"mask\", \"LSB諧調\", \"Defectラベル\"]\n",
    "                else:\n",
    "                    self.collector_log.error(\"システムに設定されていない要求仕様書バージョンです。設定されてください。\")\n",
    "\n",
    "            if \"ディフェクト\" in sheetName:\n",
    "                if bookver == \"2\":\n",
    "                    col, s_row = 2, 5\n",
    "                    strt = \"{}{}\".format(xlutil.get_column_letter(col), s_row)  # \"B5\"\n",
    "                    end = \"I{}\".format(self.__get_lastRow(ws, col, s_row))  # \"I35\"\n",
    "                    extract_col = [0, 7]\n",
    "                    col_names = [\"Defectラベル\", \"テストラベル\"]\n",
    "                else:\n",
    "                    self.collector_log.error(\"システムに設定されていない要求仕様書バージョンです。設定されてください。\")\n",
    "\n",
    "            spec_data = []\n",
    "            for row in ws[strt:end]:\n",
    "                temp_list = []\n",
    "                for i, cell in enumerate(row):\n",
    "                    if i in extract_col:\n",
    "                        temp_list.append(cell.value)\n",
    "                spec_data.append(temp_list)\n",
    "\n",
    "            spec_df = pd.DataFrame(data=spec_data, columns=col_names)\n",
    "            spec_df = spec_df.applymap(self.__convert_FullToHalfWidth)\n",
    "            spec_df = spec_df.fillna(\"-\")\n",
    "\n",
    "            return spec_df, bookver\n",
    "\n",
    "        except Exception as er:\n",
    "            self.collector_log.error(f\"要求仕様書の間違い:{str(er)} 詳細: {traceback.format_exc()}\")\n",
    "            return pd.DataFrame(columns=[\"テストラベル\", \"Defectラベル\"]), None\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df, l = extract_sheetData(\"規格（撮像）\")\n",
    "SHEETS = [shnm for shnm in xlsFile.sheet_names if \"規格\" in shnm] + [\"ディフェクト\"]\n",
    "spec_df, defct_df = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "for sh in SHEETS:\n",
    "    try:\n",
    "        tempSpecdf, tempDefctdf  = extract_sheetData(xlsFile, sh)\n",
    "        if not spec_df.empty:\n",
    "            print(f\"{sh} spec :{tempSpecdf[(tempSpecdf['テストラベル'].isin(spec_df['テストラベル']))]['テストラベル'].to_list()}\")\n",
    "            tempSpecdf = tempSpecdf[(~tempSpecdf['テストラベル'].isin(spec_df['テストラベル']))]\n",
    "            \n",
    "        if not defct_df.empty:\n",
    "            print(f\"{sh} defect :{tempDefctdf[(tempDefctdf['テストラベル'].isin(defct_df['テストラベル']))]['テストラベル'].to_list()}\")\n",
    "            tempDefctdf = tempDefctdf[(~tempDefctdf['テストラベル'].isin(defct_df['テストラベル']))]\n",
    "\n",
    "        if not tempSpecdf.empty: \n",
    "            spec_df = pd.concat([spec_df, tempSpecdf], ignore_index=True)\n",
    "        if not tempDefctdf.empty: \n",
    "            defct_df = pd.concat([defct_df, tempDefctdf], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"{sh}シートの例外:{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec_df[spec_df[\"テストラベル\"]==\"OF_ZP90\"]\n",
    "# spec_df[spec_df[\"テストラベル\"]==\"OF_VLN\"]\n",
    "# val = spec_df[spec_df[\"テストラベル\"]==\"OF_13F\"][\"パラメータ/式\"].item()\n",
    "# val = re.split(\";\",val)\n",
    "# testLabel = [re.findall(\"\\w+\", re.split(\"単独点\",v)[-1])[0] for v in val if \"単独点\" in v][0]\n",
    "# testLabel\n",
    "# spec_df\n",
    "compareCon0 = lambda lst, sublst: \"〇\" if (lst[0]<sublst[0]) and (lst[1]>sublst[1]) else \"×\"\n",
    "compareCon1 = lambda lst, sublst: \"〇\" if (lst[0]>sublst[0]) and (lst[1]<sublst[1]) else \"×\"\n",
    "\n",
    "slc = \"0.462＞D＞0.378\"\n",
    "lsb = \"0.448＞D＞0.392\"\n",
    "slceConlst = list(map(float, re.findall(r'-?\\d+\\.?\\d*', slc)))\n",
    "lsbConlst = list(map(float, re.findall(r'-?\\d+\\.?\\d*', lsb)))\n",
    "print(slceConlst)\n",
    "print(lsbConlst)\n",
    "if re.findall(\"[<＜≦]\",lsb):\n",
    "    print(compareCon0(slceConlst, lsbConlst))\n",
    "elif re.findall(\"[＞≧>]\",lsb):\n",
    "    print(compareCon1(slceConlst, lsbConlst))\n",
    "else:\n",
    "    print(\"×\")\n",
    "\n",
    "t = {\"A\":3,\"B\":7}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rwLevel = \"D≦DKT_ZIC*0.9\"\n",
    "# rwLevel = \"0.9*DKT_ZIC≦D\"\n",
    "# rwLevel = \"D≦0.9*DKT_ZIC\"\n",
    "rwLevel = \"DKT_ZIC*0.9≦D\"\n",
    "templabel = [i for i in re.findall(\"[A-z]+\", rwLevel) if i!=\"D\"][0]\n",
    "rwLvlLst = list(filter(len, re.split(templabel, rwLevel)))\n",
    "\n",
    "get_indexes = lambda ch, llst: [i for i, e in enumerate(llst) if ch in e][0]\n",
    "\n",
    "if templabel:\n",
    "    tempLvl = spec_df[spec_df[\"テストラベル\"] == templabel][\"スライスレベル\"]\n",
    "    if not tempLvl.empty:\n",
    "        tempLvl = tempLvl.item()\n",
    "        value = re.findall(r'-?\\d+\\.?\\d*', tempLvl)[0]\n",
    "\n",
    "        d_posn = get_indexes(\"D\",rwLvlLst)\n",
    "        symbl = re.findall('[/*%+-]', rwLevel)[0] # find calculation symbol\n",
    "        cal_val = rwLvlLst[get_indexes(symbl,rwLvlLst)]\n",
    "\n",
    "        if len(rwLvlLst) != 2:\n",
    "            val = re.findall(r'-?\\d+\\.?\\d*', cal_val)[0]\n",
    "            sub_lst = cal_val.split(val)\n",
    "            rwLvlLst.remove(cal_val)\n",
    "            reasmbl_idx = get_indexes(\"D\",sub_lst)\n",
    "            if reasmbl_idx:\n",
    "                cal_val = val + symbl\n",
    "                d_posn +=1\n",
    "                rwLvlLst += [cal_val, sub_lst[reasmbl_idx]]\n",
    "            else:\n",
    "                cal_val = symbl + val\n",
    "                rwLvlLst= [sub_lst[reasmbl_idx], cal_val] + rwLvlLst\n",
    "\n",
    "        if d_posn != 0:\n",
    "            trueVal = eval(cal_val + value)\n",
    "            rwLevel = str(trueVal) + rwLvlLst[d_posn]\n",
    "        else:\n",
    "            trueVal = eval(value + cal_val)\n",
    "            rwLevel = rwLvlLst[d_posn] + str(trueVal)\n",
    "        \n",
    "        print(rwLevel)\n",
    "    else:\n",
    "        print(\"does not exist in youkyu\")\n",
    "    \n",
    "else:\n",
    "    print(f\"「{rwLevel}」\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "d = pd.DataFrame({\"A\":[1,2,3,4,5], \"B\":[3,4,5,6,7], \"C\":[4,3,5,6,7]})\n",
    "# d.loc[0,[\"A\",\"B\"]] = [10,30]\n",
    "# d[\"A\"].iloc[0] = 11\n",
    "# first_column = d.pop('B')\n",
    "# d.insert(0, 'BB', first_column)\n",
    "\n",
    "lvl = \"2<D<5\"\n",
    "# lvl = \"-\"\n",
    "\n",
    "f1 = lambda x,y: \"1\" if len(str(x)) and len(str(y)) else \"▲\"\n",
    "f2 = lambda x,y: \"\" if x>3 or y>3 else \"●\"\n",
    "cn = lambda x: f1(x[\"A\"],x[\"C\"])+f2(x[\"A\"],x[\"C\"])\n",
    "ll = lambda col: False if not col else (eval(lvl.replace(\"D\",str(col))))\n",
    "t = lambda x: ll(x[\"A\"]) if lvl != \"-\" else False \n",
    "# d[\"D\"] = d.apply(cn, axis=1)\n",
    "d[\"T\"] = d.apply(t, axis=1)\n",
    "d\n",
    "\n",
    "# col = 4\n",
    "for iv in  re.findall(r'-?\\d+\\.?\\d*', lvl):\n",
    "    lvl = lvl.replace(iv, str(float(iv)*1000))\n",
    "# bool(not \"None\")\n",
    "lvl\n",
    "\n",
    "k = \"sadfa_asfas_sdfsdf_12\"\n",
    "k.rsplit(\"_\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get LSB data\n",
    "lsb_index = [2, 4, 5, 6, 7]\n",
    "lsbNmaes = [\"LSB換算名\", \"LSB諧調\", \"LSBGain\", \"LSBUnit\", \"1LSB出力\"]\n",
    "lsbpath = r\"C:\\Users\\SPPC-186\\Desktop\\Test_Files\\input\\LSB.xlsx\"\n",
    "# lsbpath = r\"C:\\Users\\SPPC-186\\Desktop\\Test_Files\\input\\IMX510_S撮測定要求仕様書_第3版(ES第3版)_縦筋改善レジスタ測定用.xlsm\"\n",
    "\n",
    "lsbdf = pd.DataFrame(columns=lsbNmaes) #xlsFile\n",
    "try:\n",
    "    tempdf = pd.read_excel(lsbpath, sheet_name=\"LSB換算値\", skiprows=range(0, 4), header=None, converters={'LSB諧調':str,'LSBGain':str}) \n",
    "    if not len(tempdf.iloc[0].dropna().to_list()):\n",
    "        raise ValueError(\"LSB換算値シートのフォーマットが違う。\")\n",
    "    \n",
    "    lsbdf = tempdf.iloc[1:,lsb_index].fillna(\"-\")\n",
    "    lsbdf.columns = lsbNmaes\n",
    "    lsbdf = lsbdf[lsbdf[lsbNmaes[0]] != \"-\"]\n",
    "    # lsbdf[\"1LSB出力\"] = lsbdf[\"1LSB出力\"].apply(lambda x: str(round(float(x) / 1000, 4)))\n",
    "except Exception as ex:\n",
    "    print(f\"LSB換算値シートの例外、詳細:{ex}\")\n",
    "\n",
    "lsbdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from glob import glob\n",
    "sokan_filePath = r\"C:\\Users\\SPPC-186\\Desktop\\Test_Files\\correlation\\\\項目_IMAGE5_ES1_Wf6_SKMBPC55_T_201801152300.XLSM\"\n",
    "# newSokanOutput_dir = os.path.dirname(newSokan_filePath)\n",
    "# dirname = os.path.split(newSokan_filePath)\n",
    "\n",
    "tempdir_name = \"理由付けアウトプット\"\n",
    "dir_name, base_name = os.path.split(sokan_filePath)\n",
    "newSokanOutput_dir = os.path.join(dir_name, tempdir_name)\n",
    "newSokan_filePath = os.path.join(newSokanOutput_dir, base_name)\n",
    "\n",
    "lastDate = datetime.date.fromtimestamp(int(os.path.getmtime(newSokan_filePath))).strftime(\"%Y%m%d%H%M%S\")\n",
    "fileExtns = os.path.splitext(newSokan_filePath)\n",
    "flpatten = f\"{fileExtns[0]}_{lastDate}_\"\n",
    "tempFiles = [fi for fi in glob(os.path.join(newSokanOutput_dir,\"*.xlsm\")) if flpatten in fi]\n",
    "os.rename(newSokan_filePath, f\"{flpatten}{str(len(tempFiles))}{fileExtns[1]}\")\n",
    "# tempFiles =sorted([(fi, int(os.path.splitext(fi)[0].rsplit(\"_\",1)[-1])) for fi in glob(os.path.join(newSokanOutput_dir,\"*.xlsm\")) if flpatten in fi], key=lambda x: x[1])\n",
    "# for fi, num in  tempFiles[::-1]:\n",
    "#     os.rename(fi, f\"{flpatten}{str(num+1)}{fileExtns[1]}\")\n",
    "\n",
    "###########################\n",
    "# tempFiles =sorted([(fi, int(os.path.splitext(fi)[0].rsplit(\"_\",1)[-1])) for fi in glob(os.path.join(newSokanOutput_dir,\"*.xlsm\")) if flpatten in fi], key=lambda x: x[1])\n",
    "# for fi, num in  tempFiles[::-1]:\n",
    "#     os.rename(fi, f\"{flpatten}{str(num+1)}{fileExtns[1]}\")\n",
    "# os.rename(newSokan_filePath, f\"{flpatten}0{fileExtns[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "g = pd.DataFrame({\"A\":[\"1\",\"12\",\"3\",\"14\",\"5\"], \"B\":[2,3,4,5,6]})\n",
    "k = pd.DataFrame({\"A\":[3,4], \"B\":[12,13]})\n",
    "k.loc[0, [\"G\"]]=\"\"\n",
    "k.loc[0, [\"G\"]]+=\"\"\n",
    "k\n",
    "# g.sort_values(by=[\"A\"])\n",
    "# ff = pd.concat([g,k], ignore_index=True)\n",
    "# ff = ff.sort_values(\"A\", inplace=True)\n",
    "# ff.drop_duplicates(subset=['A'], ignore_index=True)\n",
    "# ff.drop_duplicates(subset =\"A\", keep = False, inplace = True)\n",
    "# g[~g[\"A\"].str.contains(\"1\")]\n",
    "# import concurrent.futures\n",
    "\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# def foo(bar):\n",
    "#     time.sleep(10)\n",
    "#     print('hello {}'.format(bar))\n",
    "#     return 'foo'\n",
    "\n",
    "# print(\"start\")\n",
    "# with ThreadPoolExecutor() as executor:\n",
    "#     future = executor.submit(foo, 'world!')\n",
    "#     return_value = future.result()\n",
    "#     print(return_value)\n",
    "\n",
    "# print(\"end\")\n",
    "ggg=\",\".join([\"2\",\"3\",\"4\"]+[\"6\",\"8\"])+\";\"+ \",\".join([\"24\",\"34\",\"44\"])\n",
    "ggg.split(\";\")\n",
    "\",\".join([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsb_operator(details_df, lsb_diff):\n",
    "    unitMultipliers = {\"mv\":10**-3, \"v\":10**-6} \n",
    "    unitConvertor = lambda x, lsbdiff, umtplr: str(round((float(x)*lsbdiff)*umtplr, 4))\n",
    "\n",
    "    lsbOutput = None\n",
    "\n",
    "    if details_df[\"LSB換算名\"].item() != \"-\":\n",
    "        temp = pd.merge(lsbdf, details_df, how=\"inner\", on=[\"LSB換算名\", \"LSB諧調\",\"LSBGain\"])\n",
    "        if not temp.empty:\n",
    "            unit_ = str(temp[\"Unit\"].item()).lower()\n",
    "            if unit_ in unitMultipliers:\n",
    "                lsbOutput = unitConvertor(temp[\"1LSB出力\"].item(), lsb_diff, unitMultipliers[unit_])\n",
    "            else:\n",
    "                print(\"do not know how to chage the units\")\n",
    "        else:\n",
    "            print(\"LSB換算値が存在しません\")\n",
    "    else:\n",
    "        temp = pd.merge(lsbdf, details_df, how=\"inner\", on=[\"LSB諧調\",\"LSBGain\"])\n",
    "        if not temp.empty and len(temp)==1:\n",
    "            unit_ = str(temp[\"Unit\"].item()).lower()\n",
    "            if unit_ in unitMultipliers:\n",
    "                lsbOutput = unitConvertor(temp[\"1LSB出力\"].item(), lsb_diff, unitMultipliers[unit_])\n",
    "            else:\n",
    "                print(\"do not know how to chage the units\")\n",
    "        else:\n",
    "            img = re.split(\"[,|、]\", details_df[\"image\"].item())\n",
    "            if len(img) and img[0] != \"-\" :\n",
    "                lsbOutput = lsb_operator(spec_df[spec_df[\"テストラベル\"] == img[0]], lsb_diff)\n",
    "    \n",
    "    return lsbOutput\n",
    "\n",
    "\n",
    "lsb_diff = 2\n",
    "test = spec_df[spec_df[\"テストラベル\"] == \"DGS1_ZIC\"]\n",
    "# test[[\"LSB換算名\",\"LSB諧調\",\"LSBGain\"]] = [\"Normal_12bit\", 12, 4.2]  \n",
    "gg = lsb_operator(test, lsb_diff)\n",
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "\n",
    "writer = pd.ExcelWriter(\"testtt.xlsx\", engine='xlsxwriter')\n",
    "workbook = xlsxwriter.Workbook(writer)\n",
    "\n",
    "for sh in [\"感度ムラ\", \"理由付け\"]:\n",
    "    if sh not in writer.sheets.keys():\n",
    "        print(sh)\n",
    "        pd.DataFrame({\"A\":[1,2,3,3],\"B\":[1,2,3,4]}).to_excel(writer, sheet_name=sh)\n",
    "        workbook.set_border(1)\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openpyxl\n",
    "\n",
    "def nonEmpty_cols(ws, mrow=0, mcol=0):\n",
    "    count = 0\n",
    "    for col in ws.iter_cols(min_row=mrow, min_col=mcol, max_row=mrow):\n",
    "        for cell in col:\n",
    "            if cell.value is None:\n",
    "                return count\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def nonEmpty_rows(ws, mrow=0, mcol=0):\n",
    "    count = 0\n",
    "    for row in ws.iter_rows(min_row=mrow, min_col=mcol, max_col=mcol):\n",
    "        for cell in row:\n",
    "            if cell.value is None:\n",
    "                return count\n",
    "        count += 1\n",
    "    return 0\n",
    "\n",
    "def extractionMatrix(ws):\n",
    "    rrow, rcol, crow, ccol = 8, 1, 1, 8\n",
    "    data_rows = nonEmpty_rows(ws, rrow, rcol) + rrow - 1\n",
    "    data_cols = openpyxl.utils.get_column_letter(nonEmpty_cols(ws, crow, ccol) + ccol - 1)\n",
    "    t = [[\"A8\", \"E{}\".format(data_rows)],  # test label details\n",
    "            [\"H1\", \"{}{}\".format(data_cols, 5)],  # wafer chip data ##########################\n",
    "            [\"H8\", \"{}{}\".format(data_cols, data_rows)]]  # test data\n",
    "    return t\n",
    "\n",
    "def extract_data(workbook, sheetName) -> dict:\n",
    "\n",
    "    workbook = openpyxl.load_workbook(os.path.join(os.getcwd(), workbook))\n",
    "    worksheet = workbook[sheetName]\n",
    "\n",
    "    exmtrix = extractionMatrix(worksheet)\n",
    "\n",
    "    dataLg = []\n",
    "    for row in worksheet[exmtrix[0][0]:exmtrix[2][1]]:\n",
    "        dataLg.append([cell.value for cell in row])\n",
    "\n",
    "    col_names = []\n",
    "    temp_list = []\n",
    "    key_list = [\"type\", \"unit\", \"low\", \"high\"]\n",
    "    for i, (row, key) in enumerate(zip(worksheet[exmtrix[1][0]:exmtrix[1][1]], key_list)):\n",
    "        for j, cell in enumerate(row):\n",
    "            if j % 2 == 0:\n",
    "                if i == 0:\n",
    "                    temp_list.append({key: cell.value})\n",
    "                    col_names.extend([cell.value + \"_基準\", cell.value + \"_立上\"])\n",
    "                else:\n",
    "                    temp_list[int(j / 2)].update({key: cell.value})\n",
    "\n",
    "    data_log = {\"details\": temp_list}\n",
    "\n",
    "    dataLg_df = pd.DataFrame(dataLg)\n",
    "    dataLg_df = dataLg_df.drop(columns=[5, 6])\n",
    "    dataLg_df.columns = [\"wafer1\", \"wafer2\", \"chip_no\", \"detail1\", \"detail2\"] + col_names\n",
    "\n",
    "    data_log.update({\"testData\": dataLg_df})\n",
    "    return data_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI 活用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcn(text):\n",
    "    return text.translate(str.maketrans({chr(0xFF01 + i): chr(0x21 + i) for i in range(94)}))\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"A\":[\"     Test      :PHL_CNVERR[]          \",\n",
    "                        \"     Test      :PHL_CNVSENGB[mV]      \",\n",
    "                        \"     Test      :PHL_CNVSENB[mV]      \",\n",
    "                        \"     Test      :PHLU_CNVERR[]         \"],\n",
    "                    \"G\":[\"Test      :PHL_CNVSENGR[mV]         \",\n",
    "                        \"Test      :PHL_CNVSENR[mV]          \",\n",
    "                        \"Test      :PHL_CNV[uV]              \",\n",
    "                        \"Test      :PHLU_CNVSENGR[mV]        \"],\n",
    "                    \"row-index\":[5, 40, 76, 111]})\n",
    "df[[\"A\",\"G\"]] = df[[\"A\",\"G\"]].applymap(lambda x: x.split(\":\")[1].split(\"[\")[0])\n",
    "df\n",
    "# df1 = df.drop(columns=[\"G\"])\n",
    "# a_col_v = df1[df1[\"A\"]==\"PHL_CNVERR\"][\"row-index\"]\n",
    "# if not a_col_v.empty:\n",
    "#     print(a_col_v.iloc[0])\n",
    "\n",
    "def ttt(kk):\n",
    "    return lambda name: f\"{name} test: {kk}\"\n",
    "\n",
    "ttt(\"Ashan\")(\"tttt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(seq1, seq2):\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "\n",
    "    return (matrix[size_x - 1, size_y - 1])\n",
    "\n",
    "def sorted_levenshtein_rate(seq1, seq2):\n",
    "    product1 = ''.join(sorted(seq1))\n",
    "    product2 = ''.join(sorted(seq2))\n",
    "    distance = levenshtein(product1, product2)\n",
    "    max_len = max(len(product1), len(product2))\n",
    "    return 1-(distance/max_len)\n",
    "\n",
    "def levenshtein_rate(product1, product2):\n",
    "    distance = levenshtein(product1, product2)\n",
    "    max_len = max(len(product1), len(product2))\n",
    "    return 1 - (distance / max_len)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    met1 = 'HLC1_YFRMMX_Z2'\n",
    "    met11 = 'HLC1_YFRMMX_Z4'\n",
    "    met2 = 'HRC1_YFRM_Z2'\n",
    "    # met2 = 'HRC1_YFRMMX_' 3\n",
    "    # met2 = 'KLC1_YFRMMX_Z2D'\n",
    "    # met2 = 'DLC1_YFRMMX_Z2D'\n",
    "    print('Levenshtein Distance: {}, MatchScore: {} '.format(levenshtein(met1, met2), levenshtein_rate(met1, met2)))\n",
    "#    print('Sorted Levenshtein Distance: ', sorted_levenshtein(met1, met2))\n",
    "    print('Sorted Levenshtein Distance: {}, MatchScore: {} '.format(levenshtein(met1, met2), sorted_levenshtein_rate(met1, met2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SpellChecker(object):\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "class SpellChecker(object):\n",
    "\n",
    "  def __init__(self, corpus_file_path):\n",
    "    with open(corpus_file_path, \"r\") as file:\n",
    "      lines = file.readlines()\n",
    "      words = []\n",
    "      for line in lines:\n",
    "        words += re.findall(r'\\w+', line.lower())\n",
    "\n",
    "    self.vocabs = set(words)\n",
    "    self.word_counts = Counter(words)\n",
    "    total_words = float(sum(self.word_counts.values()))\n",
    "    self.word_probas = {word: self.word_counts[word] / total_words for word in self.vocabs}\n",
    "\n",
    "  def _level_one_edits(self, word):\n",
    "    letters = string.ascii_lowercase\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [l + r[1:] for l,r in splits if r]\n",
    "    swaps = [l + r[1] + r[0] + r[2:] for l, r in splits if len(r)>1]\n",
    "    replaces = [l + c + r[1:] for l, r in splits if r for c in letters]\n",
    "    inserts = [l + c + r for l, r in splits for c in letters] \n",
    "\n",
    "    return set(deletes + swaps + replaces + inserts)\n",
    "\n",
    "  def _level_two_edits(self, word):\n",
    "    return set(e2 for e1 in self._level_one_edits(word) for e2 in self._level_one_edits(e1))\n",
    "\n",
    "  def check(self, word):\n",
    "    candidates = self._level_one_edits(word) or self._level_two_edits(word) or [word]\n",
    "    valid_candidates = [w for w in candidates if w in self.vocabs]\n",
    "    return sorted([(c, self.word_probas[c]) for c in valid_candidates], key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "checker = SpellChecker(\"./big.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from error import *\n",
    "k = 0\n",
    "try:\n",
    "    if k == 0:\n",
    "        raise InvalidFileEntryError(error_code=\"ERAC_001\")\n",
    "    elif k == 1:\n",
    "        raise DataCollectionException(error_code=\"ERC_001\", sheet_name=\"規格撮像\")\n",
    "    elif k == 2:\n",
    "        raise PreprocessingException(error_code=\"ERP_001\", test_labels=\"AAA,BBB,CCC\")\n",
    "    elif k == 3:\n",
    "        raise DefectInterlockingException(error_code=\"ERD_001\", test_label=\"AAAA\", msg=\"AAAAの詳細情報\")\n",
    "    else:\n",
    "        raise VariationInterlockingException(msg=\"新しいケース\")\n",
    "    \n",
    "except InvalidFileEntryError as io_e:\n",
    "    print(io_e)\n",
    "except DataCollectionException as dc_e:\n",
    "    print(dc_e)\n",
    "except PreprocessingException as p_e:\n",
    "    print(p_e)\n",
    "except DefectInterlockingException as di_e:\n",
    "    print(di_e)\n",
    "except VariationInterlockingException as vi_e:\n",
    "    print(vi_e)\n",
    "\n",
    "try:\n",
    "    1/0\n",
    "except InvalidFileEntryError:\n",
    "    print(InvalidFileEntryError(error_code=\"ERAC_005\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"nv:1000, nv/e:1000, uv:1, uv/e:1, mv:-1000, mv/e:-1000, v:-1000000, lsb:1\"\n",
    "{kv.split(\":\")[0]: int(kv.split(\":\")[1]) for kv in t.split(\",\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(Exception):\n",
    "    def __init__(self, code=\"\", **kwargs:object):\n",
    "        self.kwargs = kwargs\n",
    "        self.code=code\n",
    "        super().__init__(code)\n",
    "        \n",
    "    def __str__(self):\n",
    "        td={\n",
    "            \"ERA_001\": f\"「{self.kwargs['filename']}」相関ファイルに「{self.kwargs['sheetname']}」シートコピーのエラー。\",\n",
    "            \"ERA_002\": \"Test\"\n",
    "            }\n",
    "        return f\"{self.code} {td.get(self.code,'sakjfksa')}\"\n",
    "\n",
    "try:\n",
    "    # raise Test(\"ERA_001\", sheetname=\"規格撮像\", filename=\"test.xlsx\")\n",
    "    raise Test(\"ERA_002\")\n",
    "except Test as t:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# df =  pd.DataFrame({\"A\":[1,2,\"-\",4], \"B\":[\"-\",6,7,8], \"D\":[9,11,\"-\",33], \"C\":[4,44,34,\"-\"]})\n",
    "df =  pd.DataFrame({\"A\":[\"1\",\"2\",\"\",\"4\"], \"B\":[\"\",\"6\",\"7\",\"8\"], \"D\":[\"9\",\"11\",\"\",\"33\"], \"C\":[\"4\",\"44\",\"34\",\"\"]})\n",
    "\n",
    "highlight_cells = lambda x, color: np.where(x == \"\", f\"background-color: {color};\", None)\n",
    "k = [(\"AA\", \"A\"), (\"AA\", \"B\"), (\"AA\", \"C\"), (\"AA\", \"D\")]\n",
    "df.columns = pd.MultiIndex.from_tuples(k)\n",
    "df = df.style\n",
    "# df = df.applymap(highlight_cells,  color=\"#D3D3D3\", subset=df.columns.to_list()[:-1])\n",
    "# df = df.set_properties(**{\"border\":\"5px black solid !important\"})\n",
    "# df = df.set_table_styles({\"A\":[{\"selector\" :\"td\",\n",
    "#                                 \"props\": \"border: 2px solid black;\"}]})\n",
    "\n",
    "# df = df.style.applymap(lambda x: \"background-color:lime; font-size:1.5rem;\" if x[\"A\"] == \"\" else \"background-color:tomato; font-size:0.9rem;\")\n",
    "# df = pd.DataFrame([\"【凡例】\",\"● LSB諧調\", \"▲ アドレス差分\"])\n",
    "# df.to_excel(\"test.xlsx\", startrow=1, startcol=14, index=False, header=False)\n",
    "# df[\"AA\"][[\"A\",\"C\"]]\n",
    "# cl = df.columns.to_list()\n",
    "# [i for i in cl if i[1] == \"A\"]\n",
    "# df1 = df.loc[df[\"B\"] == 6]\n",
    "# df\n",
    "df\n",
    "# label_ =\"test5,test6\"\n",
    "# maskLvl =\"test2,test3,test,4\"\n",
    "# level_ = \",\".join([label_,maskLvl])\n",
    "\n",
    "\n",
    "# datetime.date.fromtimestamp(int(os.path.getmtime(newSokan_filePath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from styleframe import StyleFrame, Styler, utils\n",
    "\n",
    "table =[[1,2,3,4],[11,12,13,88],[\"Pass\",\"Fail\",\"Pass\",\"Fail\"]]\n",
    "df = pd.DataFrame(table)\n",
    "# df = df.transpose()\n",
    "headers=[\"Current_Value\",\"Previous_Value\",\"Result\",\"test\"]\n",
    "df.columns =headers\n",
    "\n",
    "# df =  pd.DataFrame({\"A\":[\"1\",\"2\",\"\",\"4\"], \"B\":[\"\",\"6\",\"7\",\"8\"], \"D\":[\"9\",\"11\",\"\",\"33\"], \"C\":[\"4\",\"44\",\"34\",\"\"]})\n",
    "# highlight_cells = lambda x, color: np.where(x == \"\", f\"background-color: {color};\", None)\n",
    "# k = [(\"AA\", \"A\"), (\"AA\", \"B\"), (\"AA\", \"C\"), (\"AA\", \"D\")]\n",
    "# df.columns = pd.MultiIndex.from_tuples(k)\n",
    "\n",
    "writer = StyleFrame.ExcelWriter(\"test.xlsx\")\n",
    "\n",
    "sf=StyleFrame(df)\n",
    "\n",
    "sf.apply_column_style(cols_to_style=df.columns, styler_obj=Styler(bg_color=utils.colors.white, bold=True, font=utils.fonts.arial,font_size=8),style_header=True)\n",
    "\n",
    "sf.apply_headers_style(styler_obj=Styler(bg_color=utils.colors.blue, bold=True, font_size=8, font_color=utils.colors.white,number_format=utils.number_formats.general, protection=False))\n",
    "\n",
    "sf.to_excel(writer, sheet_name='Shee')\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "workbook = xlsxwriter.Workbook('test.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "image_width = 300.0\n",
    "image_height = 300.0\n",
    "\n",
    "cell_width = 64.0\n",
    "cell_height = 20.0\n",
    "\n",
    "x_scale = cell_width/image_width\n",
    "y_scale = cell_height/image_height\n",
    "\n",
    "worksheet.insert_image('B2', r\"C:\\Users\\SPPC-186\\Documents\\GitLab\\main\\02_ソース\\Semicon-Defect-Inspection\\.temp\\PHL_YFRMMX_Z2.png\",\n",
    "                       {'x_scale': 0.35, 'y_scale': 0.32})\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Data': [10, 20, 30, 20, 15, 30, 45]})\n",
    "writer = pd.ExcelWriter('test.xlsx')\n",
    "\n",
    "df.to_excel(writer, sheet_name='Sheet1')\n",
    "\n",
    "# Get the xlsxwriter workbook and worksheet objects.\n",
    "workbook  = writer.book\n",
    "try:\n",
    "    ws  = workbook.add_worksheet(\"test\")\n",
    "except:\n",
    "    ws = workbook.get_worksheet_by_name(\"test\")\n",
    "\n",
    "# Insert an image.\n",
    "\n",
    "ws.write(1,1,\"test1\")\n",
    "ws.insert_image(1,2, 'logo.png')\n",
    "\n",
    "try:\n",
    "    ws  = workbook.add_worksheet(\"test\")\n",
    "except:\n",
    "    ws = workbook.get_worksheet_by_name(\"test\")\n",
    "    ws.write(25,1,\"test1\")\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-3', '-5.3', '555']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"１００\"\n",
    "text = text.translate(str.maketrans({chr(0xFF01 + i): chr(0x21 + i) for i in range(94)}))\n",
    "text\n",
    "import re\n",
    "re.findall(r\"[+-]? *(?:\\d+(?:\\.\\d*)?|\\.\\d+)\", \"-3≦D<-5.3,555\") # 26≦|D|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'1', '2', '3'\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d = pd.DataFrame({\"A\":[1,2,3],\"B\":[3,4,5],\"C\":[33,44,55]})\n",
    "c = pd.DataFrame({\"A\":[2,5],\"B\":[4,6]})\n",
    "\n",
    "pd.merge(c, d.drop(columns=[\"B\"]), how=\"left\", on=[\"A\"])\n",
    "d.drop(columns=[\"B\"])\n",
    "d\n",
    "\n",
    "str(tuple([\"1\",\"2\",\"3\"])).replace(\"(\", \"\").replace(\")\", \"\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b13b496090387198f9cbb1cc7f1defddfbc339ac8638066bda8c30f9f5014f7f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv38_sck')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
